{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the OPENAI_TEST_KEY environment variable\n",
    "api_key = os.getenv('OPENAI_TEST_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/CV_AntonioOchotorena_092024.pdf', 'page': 0}, page_content='ANTONIO OCHOTORENA  LAYNEZ  \\n+34 636 426 538  • antonioochotorena@gmail.com  • LinkedIn  • GitHub  \\n \\nSummary   \\n \\nActively seeking a full -time opportunity in data and AI in Amsterdam.  My background in consulting \\nand data science has equipped me with the skills to excel in a position where effective \\ncommunication and technical expertise are essential . What sets me apart is my strong drive to learn, \\nproactive approach to solving complex problems, and commitment to fostering a collaborative team \\nenvironment where we support and help each other to achieve shared goals.  \\n \\nExperience   \\n \\nInfosys  Instep – Global Internship Program  Bangalore, India  \\nAI Software Engineer  06/2024 – Now  \\n \\n• Developed a proprietary information retrieval module utilizing Large Language Models \\n(LLMs) and OCR technologies to extract healthcare forms, achieving substantial cost savings.  \\n• Implemented a RAG model for HR platforms during the Infosys BizHacks Hackathon, with \\nthe solution planned for future integration.  \\n  \\nMinsait Business Consulting  Madrid, Spain  \\nData Engineer  09/2021  – 08/2023  \\n  \\n• Led the successful D WH Platform migration for Beam Suntory Spain, encompassing SSIS \\ndevelopment and maintenance (SSIS, SAP and Salesforce).   \\n• Designed and implemented reporting tools to empower Beam Suntory Spain Finance, \\nOperations, and BI departments coordinating a group of  consultants.  \\n• Implemented a comprehensive documentation process resulting in a 58% faster resolution of \\nissues, enhancing efficiency and problem -solving capabilities.  \\n \\nRamón Y Cajal Hospital (IRYCIS)   Madrid, Spain  \\nData Engineer  01/2021  – 09/2021  \\n  \\n• Created a Support Decision System utilizing Deep Learning to predict cardiac diseases from 12 -\\nLead ECGs obtaining a competitive performance at the Physionet/CinC Challenge 2020 .  \\n• Developed and maintained a scalable webservice for processing thermal images from hospital \\ndatabases saving up to 30% of physicians time.  \\n• Designed Natural Language Processing (NLP) pipelines for biomedical document classification \\n(link to publication ). \\n \\nSpanish National Cancer Research Center  Madrid, Spain  \\nBiomedical Engineer  02/2019  – 05/2019  \\n  \\n Education   \\n \\nMSc Data Science University of Amsterdam  09/2023 – 06/2024  \\n \\nMSc Biomedical Engineering,  Polytechnic University of Madrid  09/2019 – 05/2021  \\n \\nBSc Biomedical Engineering,  Polytechnic University of Madrid  09/2014 – 06/2019   \\nOther competences   \\n \\n• Spanish (Native), English (C1 – Cambridge CAE), French (B1 – Alliance Française ). \\n• Python,  R, SQL, Pandas, Scikit -learn, Tensorflow, SSIS, AWS, Azure, PowerBI, Spark, Hadoop, \\nFlask, Docker , Causal Inference Modelling  and A/B testing , Informatica IDQ . \\n• Certified Professional SCRUM master, Stanford Machine Learning, Deeplearning.ai Deep \\nLearning Specialization , Hands on Hadoop, Microsoft SQL 70 -461.  ')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the pdf with the CV information\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"./data/CV_AntonioOchotorena_092024.pdf\")\n",
    "pages = loader.load()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text \n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/CV_AntonioOchotorena_092024.pdf', 'page': 0}, page_content='ANTONIO OCHOTORENA  LAYNEZ  \\n+34 636 426 538  • antonioochotorena@gmail.com  • LinkedIn  • GitHub  \\n \\nSummary   \\n \\nActively seeking a full -time opportunity in data and AI in Amsterdam.  My background in consulting \\nand data science has equipped me with the skills to excel in a position where effective \\ncommunication and technical expertise are essential . What sets me apart is my strong drive to learn,'),\n",
       " Document(metadata={'source': './data/CV_AntonioOchotorena_092024.pdf', 'page': 0}, page_content='communication and technical expertise are essential . What sets me apart is my strong drive to learn, \\nproactive approach to solving complex problems, and commitment to fostering a collaborative team \\nenvironment where we support and help each other to achieve shared goals.  \\n \\nExperience   \\n \\nInfosys  Instep – Global Internship Program  Bangalore, India  \\nAI Software Engineer  06/2024 – Now  \\n \\n• Developed a proprietary information retrieval module utilizing Large Language Models'),\n",
       " Document(metadata={'source': './data/CV_AntonioOchotorena_092024.pdf', 'page': 0}, page_content='AI Software Engineer  06/2024 – Now  \\n \\n• Developed a proprietary information retrieval module utilizing Large Language Models \\n(LLMs) and OCR technologies to extract healthcare forms, achieving substantial cost savings.  \\n• Implemented a RAG model for HR platforms during the Infosys BizHacks Hackathon, with \\nthe solution planned for future integration.  \\n  \\nMinsait Business Consulting  Madrid, Spain  \\nData Engineer  09/2021  – 08/2023'),\n",
       " Document(metadata={'source': './data/CV_AntonioOchotorena_092024.pdf', 'page': 0}, page_content='the solution planned for future integration.  \\n  \\nMinsait Business Consulting  Madrid, Spain  \\nData Engineer  09/2021  – 08/2023  \\n  \\n• Led the successful D WH Platform migration for Beam Suntory Spain, encompassing SSIS \\ndevelopment and maintenance (SSIS, SAP and Salesforce).   \\n• Designed and implemented reporting tools to empower Beam Suntory Spain Finance, \\nOperations, and BI departments coordinating a group of  consultants.'),\n",
       " Document(metadata={'source': './data/CV_AntonioOchotorena_092024.pdf', 'page': 0}, page_content='Operations, and BI departments coordinating a group of  consultants.  \\n• Implemented a comprehensive documentation process resulting in a 58% faster resolution of \\nissues, enhancing efficiency and problem -solving capabilities.  \\n \\nRamón Y Cajal Hospital (IRYCIS)   Madrid, Spain  \\nData Engineer  01/2021  – 09/2021  \\n  \\n• Created a Support Decision System utilizing Deep Learning to predict cardiac diseases from 12 -'),\n",
       " Document(metadata={'source': './data/CV_AntonioOchotorena_092024.pdf', 'page': 0}, page_content='Data Engineer  01/2021  – 09/2021  \\n  \\n• Created a Support Decision System utilizing Deep Learning to predict cardiac diseases from 12 -\\nLead ECGs obtaining a competitive performance at the Physionet/CinC Challenge 2020 .  \\n• Developed and maintained a scalable webservice for processing thermal images from hospital \\ndatabases saving up to 30% of physicians time.  \\n• Designed Natural Language Processing (NLP) pipelines for biomedical document classification \\n(link to publication ).'),\n",
       " Document(metadata={'source': './data/CV_AntonioOchotorena_092024.pdf', 'page': 0}, page_content='• Designed Natural Language Processing (NLP) pipelines for biomedical document classification \\n(link to publication ). \\n \\nSpanish National Cancer Research Center  Madrid, Spain  \\nBiomedical Engineer  02/2019  – 05/2019  \\n  \\n Education   \\n \\nMSc Data Science University of Amsterdam  09/2023 – 06/2024  \\n \\nMSc Biomedical Engineering,  Polytechnic University of Madrid  09/2019 – 05/2021  \\n \\nBSc Biomedical Engineering,  Polytechnic University of Madrid  09/2014 – 06/2019   \\nOther competences'),\n",
       " Document(metadata={'source': './data/CV_AntonioOchotorena_092024.pdf', 'page': 0}, page_content='BSc Biomedical Engineering,  Polytechnic University of Madrid  09/2014 – 06/2019   \\nOther competences   \\n \\n• Spanish (Native), English (C1 – Cambridge CAE), French (B1 – Alliance Française ). \\n• Python,  R, SQL, Pandas, Scikit -learn, Tensorflow, SSIS, AWS, Azure, PowerBI, Spark, Hadoop, \\nFlask, Docker , Causal Inference Modelling  and A/B testing , Informatica IDQ . \\n• Certified Professional SCRUM master, Stanford Machine Learning, Deeplearning.ai Deep'),\n",
       " Document(metadata={'source': './data/CV_AntonioOchotorena_092024.pdf', 'page': 0}, page_content='• Certified Professional SCRUM master, Stanford Machine Learning, Deeplearning.ai Deep \\nLearning Specialization , Hands on Hadoop, Microsoft SQL 70 -461.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store it to Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store it into a ChromaDB database\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "PERSIST_DIR = './chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "    persist_directory=PERSIST_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO create a Summariser of job requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = \"\"\"\n",
    "ABN AMRO Traineeship - Data Scientist\n",
    "At a glance\n",
    "As a data scientist trainee you will primarily learn how to translate business challenges into technical predictive models that support a better decision making. As data scientist you will be well-equipped to store and clean large amounts of data, explore data sets to identify valuable insights, build predictive models, and run data science projects from end to end. With this Traineeship you will take the first step in your career as a data scientist to develop yourself personally and professionally!\n",
    "\n",
    "Your job\n",
    " Research and develop statistical models for analysis;\n",
    "Understand company needs and devise possible solutions by collaborating with product management/owners and the DevOps team;\n",
    "Communicate results and statistical concepts to key business stakeholders;\n",
    "Use appropriate databases and project designs to optimize joint development efforts;\n",
    "Develop custom data models and algorithms;\n",
    "Build processes and tools to help monitor and analyse performance and data accuracy;\n",
    "Use predictive modelling to enhance and optimize customer experiences, revenue generation, ad targeting, and more;\n",
    "Develop company A/B testing framework and test model quality\n",
    "Your profile\n",
    "As a data scientist trainee you should have a degree in mathematics, statistics, physics, econometrics or business, with an analytics focus. You must have strong skills in math, science, programming, databases, modelling, and predictive analytics.\n",
    "\n",
    "Who are you?\n",
    "Entrepreneurial spirit, creative mind-set and capable of assessing and evaluating risks;\n",
    "Knowledge of or at least interested in technology and how this supports business improvements;\n",
    "Thrives in an international context;\n",
    "Thinks in terms of what is possible, sees opportunities and takes a solutions-oriented approach;\n",
    "Self-starter who readily assumes responsibility;\n",
    "You have a valid EU work permit. \n",
    "\n",
    "What’s your experience?\n",
    "•   Experience working with R or Python;\n",
    "•   Experience in statistical, data mining and machine learning techniques (like boosting, generalized linear models/regression, and social network analysis);\n",
    "•   Experience visualizing and presenting data;\n",
    "•   Strong written and verbal communication skills;\n",
    "•   A proactive approach, with an ability to manage multiple priorities simultaneously;\n",
    "•   No more than 3 years of work experience (internships do not count as work experience), this is a graduate programme. \n",
    "\n",
    "Nice to have:\n",
    "•   Experience working with SQL or Java;\n",
    "•   Experience with cloud services, preferably Azure Cloud Services;\n",
    "•   Experience analyzing data from third-party providers like Adobe Analytics, Google Analytics etc.;\n",
    "•   Experience working with distributed data and computing tools like Hadoop, Hive, Map/Reduce, MySQL, and Spark;\n",
    "•   Familiarity with the Agile methodology.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tech_stack\": {\n",
      "    \"R or Python\": 25,\n",
      "    \"SQL, Java\": 15,\n",
      "    \"Hadoop, Hive, Map/Reduce, MySQL, Spark, distributed data and computing tools\": 10,\n",
      "    \"Azure Cloud Services\": 10,\n",
      "    \"Adobe Analytics, Google Analytics\": 10\n",
      "  },\n",
      "  \"soft_skills\": {\n",
      "    \"Entrepreneurial spirit, risk assessment\": 15,\n",
      "    \"Creative mindset\": 10,\n",
      "    \"International context\": 10,\n",
      "    \"Problem-solving, solutions-oriented approach\": 15,\n",
      "    \"Self-starter, responsibility\": 15,\n",
      "    \"Effective communication\": 15,\n",
      "    \"Proactive, multitasking\": 10\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "job_offer_prompt = PromptTemplate(\n",
    "    input_variables=[\"job_description\"],\n",
    "    template=\"\"\"\n",
    "    You are a professional job offer analyzer. Your task is to extract every tool, technology, and skill mentioned in the job description, grouping them into tech stack and soft skills categories. Follow these instructions carefully:\n",
    "\n",
    "    1. Use ONLY the EXACT WORDS and phrases from the job description.\n",
    "    2. Group related technologies or skills, separating them with commas if they appear separately in the text.\n",
    "    3. Assign relevancy points on a scale of 1-100 for each extracted item.\n",
    "    4. Ensure the total points for all items add up to 100.\n",
    "    5. Do not invent or infer any skills or technologies not explicitly mentioned.\n",
    "    6. Return the results in the following JSON format:\n",
    "\n",
    "    {{\n",
    "      \"tech_stack\": {{\n",
    "        \"Technology/Tool 1\": Points (1-100),\n",
    "        \"Technology/Tool 2, Related Technology\": Points (1-100),\n",
    "        ...\n",
    "      }},\n",
    "      \"soft_skills\": {{\n",
    "        \"Soft Skill 1\": Points (1-100),\n",
    "        \"Soft Skill 2\": Points (1-100),\n",
    "        ...\n",
    "      }}\n",
    "    }}\n",
    "\n",
    "    Job Description: {job_description}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def extract_job_requirements(job_description, api_key):\n",
    "    # Use ChatOpenAI as the LLM for Langchain\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=api_key)\n",
    "    \n",
    "    # Create an LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=job_offer_prompt)\n",
    "    \n",
    "    # Run the chain with the job description\n",
    "    result = chain.run(job_description)\n",
    "    \n",
    "    # Parse the result as JSON\n",
    "    try:\n",
    "        parsed_result = json.loads(result)\n",
    "        return parsed_result\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse the result as JSON\"}\n",
    "\n",
    "# Ensure API key is set\n",
    "api_key = os.getenv(\"OPENAI_TEST_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n",
    "\n",
    "result = extract_job_requirements(job_description=job_description, api_key=api_key)\n",
    "\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = f\"\"\"\n",
    "Use the following pieces of context to create a cover letter for the following job offer. \n",
    "{offer_requirements}\n",
    "If the applicant has knowledge gaps from the job offer: \n",
    "    1. Compare it to similar skills he has.\n",
    "    2. If there are no similar exclude them from the cover letter and metion them at the end.\n",
    "\n",
    "Don't try to make up an answer. \n",
    "Use a letter format with three paragraphs maximum. \n",
    "Keep the answer clear, concise and semi-formal as possible.\n",
    "Do not over extend with adjectives.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question\n",
    "{context}\n",
    "Question: {question}\n",
    "Cover letter: Fill in the letter here\n",
    "Knowledge Gaps: Add Knowledge Gaps if Any\n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0, api_key = os.getenv(\"OPENAI_TEST_KEY\"))\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antonio\\AppData\\Local\\Temp\\ipykernel_12728\\4094420968.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  result = qa_chain({\"query\": question})\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Hiring Manager,\n",
      "\n",
      "I am writing to express my interest in the AI, MLOps, and Backend Developer position at your company. With my experience as an AI Software Engineer at Infosys, where I developed proprietary information retrieval modules using Large Language Models, I believe I have the technical expertise required for this role. My proactive approach to problem-solving and commitment to fostering a collaborative team environment align well with the values of your organization.\n",
      "\n",
      "In my role at Infosys, I successfully implemented a RAG model for HR platforms during a hackathon, showcasing my ability to work on complex projects and deliver innovative solutions. Additionally, my certification as a Professional SCRUM master and completion of Stanford Machine Learning and Deeplearning.ai Deep Learning Specialization courses demonstrate my dedication to continuous learning and growth in the field of AI.\n",
      "\n",
      "While I do not have direct experience in MLOps, I am confident in my ability to quickly learn and adapt to new technologies and processes. My background in data engineering at Minsait Business Consulting in Madrid has equipped me with the necessary skills to excel in a backend developer role, where I can leverage my experience with data processing and analysis.\n",
      "\n",
      "Thank you for considering my application. I am excited about the opportunity to contribute to your team and further develop my skills in AI, MLOps, and backend development.\n",
      "\n",
      "Sincerely,\n",
      "Antonio Ochotorena Laynez\n",
      "\n",
      "Knowledge Gaps: MLOps\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"\\nUse the following pieces of context to create a cover letter for the following job offer. \\n\\n<req1> AI experience </req1>\\n<req2> MLOps </req2>\\n<req3> Backend developer </req3>\\n\\nIf the applicant has knowledge gaps from the job offer: \\n    1. Compare it to similar skills he has.\\n    2. If there are no similar exclude them from the cover letter and metion them at the end.\\n\\nDon't try to make up an answer. \\nUse a letter format with three paragraphs maximum. \\nKeep the answer clear, concise and semi-formal as possible.\\nDo not over extend with adjectives.\\n\",\n",
       " 'result': 'Dear Hiring Manager,\\n\\nI am writing to express my interest in the AI, MLOps, and Backend Developer position at your company. With my experience as an AI Software Engineer at Infosys, where I developed proprietary information retrieval modules using Large Language Models, I believe I have the technical expertise required for this role. My proactive approach to problem-solving and commitment to fostering a collaborative team environment align well with the values of your organization.\\n\\nIn my role at Infosys, I successfully implemented a RAG model for HR platforms during a hackathon, showcasing my ability to work on complex projects and deliver innovative solutions. Additionally, my certification as a Professional SCRUM master and completion of Stanford Machine Learning and Deeplearning.ai Deep Learning Specialization courses demonstrate my dedication to continuous learning and growth in the field of AI.\\n\\nWhile I do not have direct experience in MLOps, I am confident in my ability to quickly learn and adapt to new technologies and processes. My background in data engineering at Minsait Business Consulting in Madrid has equipped me with the necessary skills to excel in a backend developer role, where I can leverage my experience with data processing and analysis.\\n\\nThank you for considering my application. I am excited about the opportunity to contribute to your team and further develop my skills in AI, MLOps, and backend development.\\n\\nSincerely,\\nAntonio Ochotorena Laynez\\n\\nKnowledge Gaps: MLOps',\n",
       " 'source_documents': [Document(metadata={'page': 0, 'source': './data/CV_AntonioOchotorena_092024.pdf'}, page_content='communication and technical expertise are essential . What sets me apart is my strong drive to learn, \\nproactive approach to solving complex problems, and commitment to fostering a collaborative team \\nenvironment where we support and help each other to achieve shared goals.  \\n \\nExperience   \\n \\nInfosys  Instep – Global Internship Program  Bangalore, India  \\nAI Software Engineer  06/2024 – Now  \\n \\n• Developed a proprietary information retrieval module utilizing Large Language Models'),\n",
       "  Document(metadata={'page': 0, 'source': './data/CV_AntonioOchotorena_092024.pdf'}, page_content='AI Software Engineer  06/2024 – Now  \\n \\n• Developed a proprietary information retrieval module utilizing Large Language Models \\n(LLMs) and OCR technologies to extract healthcare forms, achieving substantial cost savings.  \\n• Implemented a RAG model for HR platforms during the Infosys BizHacks Hackathon, with \\nthe solution planned for future integration.  \\n  \\nMinsait Business Consulting  Madrid, Spain  \\nData Engineer  09/2021  – 08/2023'),\n",
       "  Document(metadata={'page': 0, 'source': './data/CV_AntonioOchotorena_092024.pdf'}, page_content='ANTONIO OCHOTORENA  LAYNEZ  \\n+34 636 426 538  • antonioochotorena@gmail.com  • LinkedIn  • GitHub  \\n \\nSummary   \\n \\nActively seeking a full -time opportunity in data and AI in Amsterdam.  My background in consulting \\nand data science has equipped me with the skills to excel in a position where effective \\ncommunication and technical expertise are essential . What sets me apart is my strong drive to learn,'),\n",
       "  Document(metadata={'page': 0, 'source': './data/CV_AntonioOchotorena_092024.pdf'}, page_content='• Certified Professional SCRUM master, Stanford Machine Learning, Deeplearning.ai Deep \\nLearning Specialization , Hands on Hadoop, Microsoft SQL 70 -461.')]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_completion(prompt, client):\n",
    "    \n",
    "#     completion = client.chat.completions.create(\n",
    "#                     model=\"gpt-3.5-turbo-0125\",\n",
    "#                     messages=[\n",
    "#                         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#                         {\"role\": \"user\", \"content\": prompt}\n",
    "#                     ],\n",
    "#                     max_tokens=1000,\n",
    "#                     temperature=0,\n",
    "#                 )\n",
    "#     return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pages[0].page_content\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Please follow the following format:\n",
    "<role 1> <reasons based on experience> <areas of improvement towards role 1>\n",
    "<role 2> <reasons based on experience> <areas of improvement towards role 2>\n",
    "<role 3> <reasons based on experience> <areas of improvement towards role 3>\n",
    "\n",
    "Do not do more than 3 roles\n",
    "```{text}```\n",
    "\"\"\"\n",
    "result = get_completion(prompt, client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Engineer at Infosys Instep in Amsterdam\n",
      "Antonio has a strong background in data engineering, as evidenced by his successful DWH platform migration for Beam Suntory Spain and the development of a Support Decision System using Deep Learning at Ramón Y Cajal Hospital. He has experience in leading projects, implementing reporting tools, and designing NLP pipelines for biomedical document classification. Antonio's technical skills in Python, R, SQL, and various data tools make him well-suited for a Data Engineer role.\n",
      "\n",
      "Areas of improvement:\n",
      "- Antonio could focus on enhancing his knowledge of cloud platforms like AWS and Azure to stay updated with the latest technologies in data engineering.\n",
      "- Developing expertise in big data technologies like Spark and Hadoop would further strengthen his profile for data engineering roles.\n",
      "\n",
      "AI Software Engineer in Amsterdam\n",
      "Antonio's experience in developing an information retrieval module using Large Language Models and OCR technologies at Infosys Instep showcases his expertise in AI. His proactive approach to problem-solving and commitment to learning align well with the requirements of an AI Software Engineer role. Antonio's participation in hackathons and competitive challenges demonstrates his ability to innovate and deliver solutions in AI projects.\n",
      "\n",
      "Areas of improvement:\n",
      "- Antonio could benefit from further deepening his knowledge in advanced AI concepts like reinforcement learning and generative models to broaden his skill set as an AI Software Engineer.\n",
      "- Engaging in more collaborative AI projects or open-source contributions could help Antonio gain practical experience in deploying AI solutions at scale.\n",
      "\n",
      "Biomedical Engineer at Spanish National Cancer Research Center\n",
      "Antonio's background in biomedical engineering, particularly in developing a Support Decision System for predicting cardiac diseases and designing NLP pipelines for biomedical document classification, highlights his expertise in this field. His ability to leverage deep learning and data analysis techniques for healthcare applications positions him well for roles in biomedical engineering.\n",
      "\n",
      "Areas of improvement:\n",
      "- Antonio could explore opportunities to work on interdisciplinary projects that combine data science and biomedical engineering to further enhance his skills in both domains.\n",
      "- Pursuing additional certifications or courses in healthcare analytics or medical imaging analysis could help Antonio stay abreast of the latest advancements in biomedical engineering.\n"
     ]
    }
   ],
   "source": [
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
